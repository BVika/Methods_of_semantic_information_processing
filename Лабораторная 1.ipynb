{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeoamVJosa9O1R3gsC3Mag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BVika/Methods_of_semantic_information_processing/blob/main/%D0%9B%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Лабораторная 1"
      ],
      "metadata": {
        "id": "RqhZifT8TEsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Задание\n",
        "\n",
        "1. Провести на любом тексте лемматизацию и стемминг (nltk, pymorphy2, pymorphy3, natasha)\n",
        "2. Написать функцию для токенизации всех символов из ASCII\n",
        "3. Написать функцию для векторизации всех символов из ASCII\n",
        "4.  Провести токенизацию и векторизацию текста после лемматизации и стемминга"
      ],
      "metadata": {
        "id": "1Sfn5fYTT9SE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инсталируем библиотеку nltk"
      ],
      "metadata": {
        "id": "D_fiyCAXW3R_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cNU1SEvbTCzT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef3f2ab-4f8e-4baa-9400-3743d3f5210a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnXFk756bCsb",
        "outputId": "64475917-0cb3-4728-d08c-63a7f30134c9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем библиотеки"
      ],
      "metadata": {
        "id": "-nthHp0paMrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from pymystem3 import Mystem\n",
        "from typing import List\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "YlGH68NHjJ9N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Лемматизацию и стемминг с помощью nltk"
      ],
      "metadata": {
        "id": "LPSSpRLjjPTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab') #ресурс содержит модели для разбиения текста на предложения и слова\n",
        "\n",
        "m = Mystem()#Программа MyStem производит морфологический анализ текста на русском языке\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize(text: str) -> List[str]:\n",
        "    lemmas = m.lemmatize(text)\n",
        "    cleaned_lemmas = [lemma.strip() for lemma in lemmas if lemma.strip() and lemma not in [' ']]\n",
        "    return cleaned_lemmas\n",
        "\n",
        "# Функция для стемминга\n",
        "def steming(text: str) -> List[str]:\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Пример текста\n",
        "text = \"Он спас мне жизнь, и я смотрел на него снизу вверх с самого детства. Однако я хотел догнать его ещё больше, стать для него ещё более сильным человеком. Хотя, должно быть, он плохо меня помнит. Мы никогда по-настоящему не разговаривали. Я хочу защитить его. Он пристально посмотрел не Се Ляня: -Если твоя мечта - спасти людей, то моя мечта - это только ты.\"\n",
        "\n",
        "# Выполнение лемматизации и стемминга\n",
        "lemmatized_text = lemmatize(text)\n",
        "steming_text = steming(text)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Лемматизированный текст:\", lemmatized_text)\n",
        "print(\"Стеммированные текст:\", steming_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivzIEFHPXZIW",
        "outputId": "4498269b-7fcf-489b-e21e-fb4584f5e96b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированный текст: ['он', 'спасать', 'я', 'жизнь', ',', 'и', 'я', 'смотреть', 'на', 'он', 'снизу', 'вверх', 'с', 'самый', 'детство', '.', 'однако', 'я', 'хотеть', 'догнать', 'он', 'еще', 'много', ',', 'становиться', 'для', 'он', 'еще', 'более', 'сильный', 'человек', '.', 'хотя', ',', 'должно', 'быть', ',', 'он', 'плохо', 'я', 'помнить', '.', 'мы', 'никогда', 'по-настоящему', 'не', 'разговаривать', '.', 'я', 'хотеть', 'защищать', 'он', '.', 'он', 'пристально', 'посмотреть', 'не', 'се', 'ляня', ': -', 'если', 'твой', 'мечта', '-', 'спасать', 'человек', ',', 'то', 'мой', 'мечта', '-', 'это', 'только', 'ты', '.']\n",
            "Стеммированные текст: ['он', 'спас', 'мне', 'жизн', ',', 'и', 'я', 'смотрел', 'на', 'нег', 'сниз', 'вверх', 'с', 'сам', 'детств', '.', 'однак', 'я', 'хотел', 'догна', 'ег', 'ещ', 'больш', ',', 'стат', 'для', 'нег', 'ещ', 'бол', 'сильн', 'человек', '.', 'хот', ',', 'должн', 'быт', ',', 'он', 'плох', 'мен', 'помн', '.', 'мы', 'никогд', 'по-настоя', 'не', 'разговарива', '.', 'я', 'хоч', 'защит', 'ег', '.', 'он', 'пристальн', 'посмотрел', 'не', 'се', 'лян', ':', '-есл', 'тво', 'мечт', '-', 'спаст', 'люд', ',', 'то', 'мо', 'мечт', '-', 'эт', 'тольк', 'ты', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Функция для токенизации всех символов из ASCII"
      ],
      "metadata": {
        "id": "UqvUJXgaxK3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_ascii_characters(input_string):\n",
        "    return [char for char in input_string if ord(char) < 128] #Фильтрация символов ASCII из входной строки, исключая символы, не входящие в диапазон ASCII.\n",
        "\n",
        "text = \"Hello, Привет! 1234 @#$% \"\n",
        "ascii_characters = filter_ascii_characters(text)\n",
        "print(ascii_characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saHDWAZD_LTH",
        "outputId": "b5a8dfb2-d3d2-4528-c0a2-e574eca141d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H', 'e', 'l', 'l', 'o', ',', ' ', '!', ' ', '1', '2', '3', '4', ' ', '@', '#', '$', '%', ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Написать функцию для векторизации всех символов из ASCII"
      ],
      "metadata": {
        "id": "prVATlTOfDg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_vectorization(input_string):\n",
        "    return [ord(char) for char in input_string if ord(char) < 128]\n",
        "\n",
        "input_str = \"Hello, Привет! 1234 @#$% \"\n",
        "ascii_vector = ascii_vectorization(input_str)\n",
        "print(ascii_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysjuKY6jfDJK",
        "outputId": "e09404d9-d8b1-41e8-f27c-d4acba930ae4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72, 101, 108, 108, 111, 44, 32, 33, 32, 49, 50, 51, 52, 32, 64, 35, 36, 37, 32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Провести токенизацию и векторизацию текста после лемматизации и стемминга"
      ],
      "metadata": {
        "id": "nyqdx1Yf1Q8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7sy2dBhSdxUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "XhlrSMk61hrj"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcCm6zVfUbaQ",
        "outputId": "07389f5b-31f1-47c4-85be-15f827a15769"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#токенизация\n",
        "def preprocess(text, stop_words, punctuation_marks, morph):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    preprocessed_text = []\n",
        "    for token in tokens:\n",
        "        if token not in punctuation_marks:\n",
        "            lemma = morph.parse(token)[0].normal_form #лемматизация\n",
        "            if lemma not in stop_words:\n",
        "              stemmed = stemmer.stem(lemma)  # Стемминг\n",
        "              preprocessed_text.append(stemmed)\n",
        "    return preprocessed_text\n",
        "punctuation_marks = ['!', ',', '(', ')', ':', '-', '?', '.', '..', '...', '«', '»', ';', '–', '--', ' ']\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "Preprocessed_texts = preprocess(text, punctuation_marks, stop_words, morph)\n",
        "\n",
        "#Считаем частоту букв\n",
        "letters = Counter()\n",
        "for txt in Preprocessed_texts:\n",
        "    letters.update(txt)\n",
        "# print(words.most_common(10))\n",
        "print(letters)\n",
        "\n",
        "# Словарь, отображающий слова в коды\n",
        "letters_to_index = dict()\n",
        "# Словарь, отображающий коды в слова\n",
        "index_to_letters = dict()\n",
        "\n",
        "max_letters = 10000\n",
        "\n",
        "#Создаем словари\n",
        "for i, letter in enumerate(letters.most_common(max_letters - 2)):\n",
        "    letters_to_index[letter[0]] = i + 2\n",
        "    index_to_letters[i + 2] = letter[0]\n",
        "\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Токены:\", Preprocessed_texts)\n",
        "print(\"Словарь векторизатора:\", letters_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f0oZU5EIwTt",
        "outputId": "9db76eb3-fbe0-4dd0-eb4c-e3b9e669462f"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'т': 23, 'о': 17, 'с': 15, 'е': 15, 'а': 12, 'н': 10, 'в': 8, 'п': 7, 'и': 7, 'л': 7, 'м': 6, 'р': 6, 'х': 5, 'з': 4, 'д': 4, 'ч': 4, 'к': 3, 'щ': 3, 'ж': 2, 'г': 2, 'ь': 2, '-': 2, 'я': 2, 'э': 1})\n",
            "Токены: ['спаст', 'жизн', 'смотрет', 'сниз', 'вверх', 'сам', 'детств', 'однак', 'хотет', 'догна', 'ещ', 'стат', 'ещ', 'сильн', 'человек', 'хот', 'должн', 'плох', 'помн', 'по-настоя', 'разговарива', 'хотет', 'защит', 'пристальн', 'посмотрет', 'си', 'лян', '-ест', 'тво', 'мечт', 'спаст', 'человек', 'мечт', 'эт']\n",
            "Словарь векторизатора: {'т': 2, 'о': 3, 'с': 4, 'е': 5, 'а': 6, 'н': 7, 'в': 8, 'п': 9, 'и': 10, 'л': 11, 'м': 12, 'р': 13, 'х': 14, 'з': 15, 'д': 16, 'ч': 17, 'к': 18, 'щ': 19, 'ж': 20, 'г': 21, 'ь': 22, '-': 23, 'я': 24, 'э': 25}\n"
          ]
        }
      ]
    }
  ]
}