{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAHWVc+KKPVZire4HMszNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BVika/Methods_of_semantic_information_processing/blob/main/%D0%9B%D0%B0%D0%B11_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Лабораторная 1"
      ],
      "metadata": {
        "id": "RqhZifT8TEsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Задание\n",
        "\n",
        "1. Провести на любом тексте лемматизацию и стемминг (nltk, pymorphy2, pymorphy3, natasha)\n",
        "2. Написать функцию для токенизации всех символов из ASCII\n",
        "3. Написать функцию для векторизации всех символов из ASCII\n",
        "4.  Провести токенизацию и векторизацию текста после лемматизации и стемминга"
      ],
      "metadata": {
        "id": "1Sfn5fYTT9SE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инсталируем библиотеку nltk"
      ],
      "metadata": {
        "id": "D_fiyCAXW3R_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cNU1SEvbTCzT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0077d6af-f36d-441f-e8a6-6abf9defae72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnXFk756bCsb",
        "outputId": "5aa37094-6b4b-46cb-c34b-b907a7fd7a75"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем библиотеки"
      ],
      "metadata": {
        "id": "-nthHp0paMrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from pymystem3 import Mystem\n",
        "from typing import List\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "YlGH68NHjJ9N"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Лемматизацию и стемминг с помощью nltk"
      ],
      "metadata": {
        "id": "LPSSpRLjjPTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "на русском"
      ],
      "metadata": {
        "id": "awbDBF8auPdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab') #ресурс содержит модели для разбиения текста на предложения и слова\n",
        "\n",
        "m = Mystem()#Программа MyStem производит морфологический анализ текста на русском языке\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize(text: str) -> List[str]:\n",
        "    lemmas = m.lemmatize(text)\n",
        "    cleaned_lemmas = [lemma.strip() for lemma in lemmas if lemma.strip() and lemma not in [' ']]\n",
        "    return cleaned_lemmas\n",
        "\n",
        "# Функция для стемминга\n",
        "def steming(text: str) -> List[str]:\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Пример текста\n",
        "text = \"Он спас мне жизнь, и я смотрел на него снизу вверх с самого детства. Однако я хотел догнать его ещё больше, стать для него ещё более сильным человеком. Хотя, должно быть, он плохо меня помнит. Мы никогда по-настоящему не разговаривали. Я хочу защитить его. Он пристально посмотрел не Се Ляня: -Если твоя мечта - спасти людей, то моя мечта - это только ты.\"\n",
        "\n",
        "# Выполнение лемматизации и стемминга\n",
        "lemmatized_text = lemmatize(text)\n",
        "steming_text = steming(text)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Лемматизированный текст:\", lemmatized_text)\n",
        "print(\"Стеммированные текст:\", steming_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivzIEFHPXZIW",
        "outputId": "4498269b-7fcf-489b-e21e-fb4584f5e96b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированный текст: ['он', 'спасать', 'я', 'жизнь', ',', 'и', 'я', 'смотреть', 'на', 'он', 'снизу', 'вверх', 'с', 'самый', 'детство', '.', 'однако', 'я', 'хотеть', 'догнать', 'он', 'еще', 'много', ',', 'становиться', 'для', 'он', 'еще', 'более', 'сильный', 'человек', '.', 'хотя', ',', 'должно', 'быть', ',', 'он', 'плохо', 'я', 'помнить', '.', 'мы', 'никогда', 'по-настоящему', 'не', 'разговаривать', '.', 'я', 'хотеть', 'защищать', 'он', '.', 'он', 'пристально', 'посмотреть', 'не', 'се', 'ляня', ': -', 'если', 'твой', 'мечта', '-', 'спасать', 'человек', ',', 'то', 'мой', 'мечта', '-', 'это', 'только', 'ты', '.']\n",
            "Стеммированные текст: ['он', 'спас', 'мне', 'жизн', ',', 'и', 'я', 'смотрел', 'на', 'нег', 'сниз', 'вверх', 'с', 'сам', 'детств', '.', 'однак', 'я', 'хотел', 'догна', 'ег', 'ещ', 'больш', ',', 'стат', 'для', 'нег', 'ещ', 'бол', 'сильн', 'человек', '.', 'хот', ',', 'должн', 'быт', ',', 'он', 'плох', 'мен', 'помн', '.', 'мы', 'никогд', 'по-настоя', 'не', 'разговарива', '.', 'я', 'хоч', 'защит', 'ег', '.', 'он', 'пристальн', 'посмотрел', 'не', 'се', 'лян', ':', '-есл', 'тво', 'мечт', '-', 'спаст', 'люд', ',', 'то', 'мо', 'мечт', '-', 'эт', 'тольк', 'ты', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "на английском"
      ],
      "metadata": {
        "id": "eb0K8mBhuS21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Инициализация лемматизатора и стеммера\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize(text: str) -> List[str]:\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmas\n",
        "\n",
        "# Функция для стемминга\n",
        "def stemming(text: str) -> List[str]:\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Пример текста\n",
        "text = \"He saved my life, and I looked up to him since childhood. However, I wanted to catch up with him even more, to become an even stronger person for him. Although, he must not remember me well. We never really talked. I want to protect him. He looked closely at Se Lyan: -If your dream is to save people, then my dream is only you.\"\n",
        "\n",
        "# Выполнение лемматизации и стемминга\n",
        "lemmatized_text = lemmatize(text)\n",
        "stemmed_text = stemming(text)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Lemmatized text:\", lemmatized_text)\n",
        "print(\"Stemmed text:\", stemmed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cq6NTcSl8Zh",
        "outputId": "005ce164-5e2e-42fb-af82-ef0b336ebdee"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized text: ['He', 'saved', 'my', 'life', ',', 'and', 'I', 'looked', 'up', 'to', 'him', 'since', 'childhood', '.', 'However', ',', 'I', 'wanted', 'to', 'catch', 'up', 'with', 'him', 'even', 'more', ',', 'to', 'become', 'an', 'even', 'stronger', 'person', 'for', 'him', '.', 'Although', ',', 'he', 'must', 'not', 'remember', 'me', 'well', '.', 'We', 'never', 'really', 'talked', '.', 'I', 'want', 'to', 'protect', 'him', '.', 'He', 'looked', 'closely', 'at', 'Se', 'Lyan', ':', '-If', 'your', 'dream', 'is', 'to', 'save', 'people', ',', 'then', 'my', 'dream', 'is', 'only', 'you', '.']\n",
            "Stemmed text: ['he', 'save', 'my', 'life', ',', 'and', 'i', 'look', 'up', 'to', 'him', 'sinc', 'childhood', '.', 'howev', ',', 'i', 'want', 'to', 'catch', 'up', 'with', 'him', 'even', 'more', ',', 'to', 'becom', 'an', 'even', 'stronger', 'person', 'for', 'him', '.', 'although', ',', 'he', 'must', 'not', 'rememb', 'me', 'well', '.', 'we', 'never', 'realli', 'talk', '.', 'i', 'want', 'to', 'protect', 'him', '.', 'he', 'look', 'close', 'at', 'se', 'lyan', ':', '-if', 'your', 'dream', 'is', 'to', 'save', 'peopl', ',', 'then', 'my', 'dream', 'is', 'onli', 'you', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Функция для токенизации всех символов из ASCII"
      ],
      "metadata": {
        "id": "UqvUJXgaxK3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_ascii_characters(input_string):\n",
        "    input_string = input_string.replace(\" \", \"\")\n",
        "    return [char for char in input_string if ord(char) < 128] #Фильтрация символов ASCII из входной строки, исключая символы, не входящие в диапазон ASCII.\n",
        "\n",
        "text = \"Hello, Привет! 1234 @#$% \"\n",
        "ascii_characters = filter_ascii_characters(text)\n",
        "print(ascii_characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saHDWAZD_LTH",
        "outputId": "fed4729c-ce45-4b9f-d0f7-530197b44787"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H', 'e', 'l', 'l', 'o', ',', '!', '1', '2', '3', '4', '@', '#', '$', '%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U775ot98fHjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Написать функцию для векторизации всех символов из ASCII"
      ],
      "metadata": {
        "id": "prVATlTOfDg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_vectorization(input_string):\n",
        "    input_string = input_string.replace(\" \", \"\")\n",
        "    return [ord(char) for char in input_string if ord(char) < 128]\n",
        "\n",
        "input_str = \"Hello, Привет! 1234 @#$% \"\n",
        "ascii_vector = ascii_vectorization(input_str)\n",
        "print(ascii_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysjuKY6jfDJK",
        "outputId": "59761628-4f5f-433a-ca5b-e83b70bb7adf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72, 101, 108, 108, 111, 44, 33, 49, 50, 51, 52, 64, 35, 36, 37]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Провести токенизацию и векторизацию текста после лемматизации и стемминга"
      ],
      "metadata": {
        "id": "nyqdx1Yf1Q8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7sy2dBhSdxUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "XhlrSMk61hrj"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcCm6zVfUbaQ",
        "outputId": "07389f5b-31f1-47c4-85be-15f827a15769"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "лемматизации и стемминга"
      ],
      "metadata": {
        "id": "UMGTWhmwoNGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer_en = WordNetLemmatizer()\n",
        "stemmer_en = SnowballStemmer(\"english\")\n",
        "stemmer_ru = SnowballStemmer(\"russian\")\n",
        "\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize(text: str) -> List[str]:\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmas\n",
        "\n",
        "# Функция для стемминга\n",
        "def stemming(text: str) -> List[str]:\n",
        "    tokens = lemmatize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Пример текста\n",
        "text_ru = \"Он спас мне жизнь, и я смотрел на него снизу вверх с самого детства. Однако я хотел догнать его ещё больше, стать для него ещё более сильным человеком. Хотя, должно быть, он плохо меня помнит. Мы никогда по-настоящему не разговаривали. Я хочу защитить его. Он пристально посмотрел не Се Ляня: -Если твоя мечта - спасти людей, то моя мечта - это только ты.\"\n",
        "text_en = \"He saved my life, and I looked up to him since childhood. However, I wanted to catch up with him even more, to become an even stronger person for him. Although, he must not remember me well. We never really talked. I want to protect him. He looked closely at Se Lyan: -If your dream is to save people, then my dream is only you.\"\n",
        "\n",
        "stem_lem_ru=stemming(text_ru)\n",
        "stem_lem_en =stemming(text_en)\n",
        "print('Текст после лемматизации и стемминга: ', stem_lem_ru)\n",
        "print('Текст после лемматизации и стемминга: ',stem_lem_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmF-0mXEov6a",
        "outputId": "0856371f-c6e7-4347-d646-cee9a87d80f4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст после лемматизации и стемминга:  ['он', 'спас', 'мне', 'жизнь', ',', 'и', 'я', 'смотрел', 'на', 'него', 'снизу', 'вверх', 'с', 'самого', 'детства', '.', 'однако', 'я', 'хотел', 'догнать', 'его', 'ещё', 'больше', ',', 'стать', 'для', 'него', 'ещё', 'более', 'сильным', 'человеком', '.', 'хотя', ',', 'должно', 'быть', ',', 'он', 'плохо', 'меня', 'помнит', '.', 'мы', 'никогда', 'по-настоящему', 'не', 'разговаривали', '.', 'я', 'хочу', 'защитить', 'его', '.', 'он', 'пристально', 'посмотрел', 'не', 'се', 'ляня', ':', '-если', 'твоя', 'мечта', '-', 'спасти', 'людей', ',', 'то', 'моя', 'мечта', '-', 'это', 'только', 'ты', '.']\n",
            "Текст после лемматизации и стемминга:  ['he', 'save', 'my', 'life', ',', 'and', 'i', 'look', 'up', 'to', 'him', 'sinc', 'childhood', '.', 'howev', ',', 'i', 'want', 'to', 'catch', 'up', 'with', 'him', 'even', 'more', ',', 'to', 'becom', 'an', 'even', 'stronger', 'person', 'for', 'him', '.', 'although', ',', 'he', 'must', 'not', 'rememb', 'me', 'well', '.', 'we', 'never', 'realli', 'talk', '.', 'i', 'want', 'to', 'protect', 'him', '.', 'he', 'look', 'close', 'at', 'se', 'lyan', ':', '-if', 'your', 'dream', 'is', 'to', 'save', 'peopl', ',', 'then', 'my', 'dream', 'is', 'onli', 'you', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизация"
      ],
      "metadata": {
        "id": "q7n6F5wGtzm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorization(input_string):\n",
        "    input_string = ''.join(input_string)\n",
        "    return [ord(char) for char in input_string]\n",
        "\n",
        "\n",
        "vector_ru = vectorization(stem_lem_ru)\n",
        "vector_en = vectorization(stem_lem_en)\n",
        "print(vector_ru)\n",
        "print(vector_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps5gkMFbncjd",
        "outputId": "e7ea87fd-f353-48dd-bf88-2c4e7a04dcfd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1086, 1085, 1089, 1087, 1072, 1089, 1084, 1085, 1077, 1078, 1080, 1079, 1085, 1100, 44, 1080, 1103, 1089, 1084, 1086, 1090, 1088, 1077, 1083, 1085, 1072, 1085, 1077, 1075, 1086, 1089, 1085, 1080, 1079, 1091, 1074, 1074, 1077, 1088, 1093, 1089, 1089, 1072, 1084, 1086, 1075, 1086, 1076, 1077, 1090, 1089, 1090, 1074, 1072, 46, 1086, 1076, 1085, 1072, 1082, 1086, 1103, 1093, 1086, 1090, 1077, 1083, 1076, 1086, 1075, 1085, 1072, 1090, 1100, 1077, 1075, 1086, 1077, 1097, 1105, 1073, 1086, 1083, 1100, 1096, 1077, 44, 1089, 1090, 1072, 1090, 1100, 1076, 1083, 1103, 1085, 1077, 1075, 1086, 1077, 1097, 1105, 1073, 1086, 1083, 1077, 1077, 1089, 1080, 1083, 1100, 1085, 1099, 1084, 1095, 1077, 1083, 1086, 1074, 1077, 1082, 1086, 1084, 46, 1093, 1086, 1090, 1103, 44, 1076, 1086, 1083, 1078, 1085, 1086, 1073, 1099, 1090, 1100, 44, 1086, 1085, 1087, 1083, 1086, 1093, 1086, 1084, 1077, 1085, 1103, 1087, 1086, 1084, 1085, 1080, 1090, 46, 1084, 1099, 1085, 1080, 1082, 1086, 1075, 1076, 1072, 1087, 1086, 45, 1085, 1072, 1089, 1090, 1086, 1103, 1097, 1077, 1084, 1091, 1085, 1077, 1088, 1072, 1079, 1075, 1086, 1074, 1072, 1088, 1080, 1074, 1072, 1083, 1080, 46, 1103, 1093, 1086, 1095, 1091, 1079, 1072, 1097, 1080, 1090, 1080, 1090, 1100, 1077, 1075, 1086, 46, 1086, 1085, 1087, 1088, 1080, 1089, 1090, 1072, 1083, 1100, 1085, 1086, 1087, 1086, 1089, 1084, 1086, 1090, 1088, 1077, 1083, 1085, 1077, 1089, 1077, 1083, 1103, 1085, 1103, 58, 45, 1077, 1089, 1083, 1080, 1090, 1074, 1086, 1103, 1084, 1077, 1095, 1090, 1072, 45, 1089, 1087, 1072, 1089, 1090, 1080, 1083, 1102, 1076, 1077, 1081, 44, 1090, 1086, 1084, 1086, 1103, 1084, 1077, 1095, 1090, 1072, 45, 1101, 1090, 1086, 1090, 1086, 1083, 1100, 1082, 1086, 1090, 1099, 46]\n",
            "[104, 101, 115, 97, 118, 101, 109, 121, 108, 105, 102, 101, 44, 97, 110, 100, 105, 108, 111, 111, 107, 117, 112, 116, 111, 104, 105, 109, 115, 105, 110, 99, 99, 104, 105, 108, 100, 104, 111, 111, 100, 46, 104, 111, 119, 101, 118, 44, 105, 119, 97, 110, 116, 116, 111, 99, 97, 116, 99, 104, 117, 112, 119, 105, 116, 104, 104, 105, 109, 101, 118, 101, 110, 109, 111, 114, 101, 44, 116, 111, 98, 101, 99, 111, 109, 97, 110, 101, 118, 101, 110, 115, 116, 114, 111, 110, 103, 101, 114, 112, 101, 114, 115, 111, 110, 102, 111, 114, 104, 105, 109, 46, 97, 108, 116, 104, 111, 117, 103, 104, 44, 104, 101, 109, 117, 115, 116, 110, 111, 116, 114, 101, 109, 101, 109, 98, 109, 101, 119, 101, 108, 108, 46, 119, 101, 110, 101, 118, 101, 114, 114, 101, 97, 108, 108, 105, 116, 97, 108, 107, 46, 105, 119, 97, 110, 116, 116, 111, 112, 114, 111, 116, 101, 99, 116, 104, 105, 109, 46, 104, 101, 108, 111, 111, 107, 99, 108, 111, 115, 101, 97, 116, 115, 101, 108, 121, 97, 110, 58, 45, 105, 102, 121, 111, 117, 114, 100, 114, 101, 97, 109, 105, 115, 116, 111, 115, 97, 118, 101, 112, 101, 111, 112, 108, 44, 116, 104, 101, 110, 109, 121, 100, 114, 101, 97, 109, 105, 115, 111, 110, 108, 105, 121, 111, 117, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токинизация"
      ],
      "metadata": {
        "id": "tYLGSwryt4ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokeniz(input_string):\n",
        "    input_string = ''.join(input_string)\n",
        "    return [char for char in input_string]\n",
        "\n",
        "\n",
        "vector_ru = tokeniz(stem_lem_ru)\n",
        "vector_en = tokeniz(stem_lem_en)\n",
        "print(vector_ru)\n",
        "print(vector_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Zedc5d0tk3o",
        "outputId": "815ebf34-e0c8-4fdc-8e92-af58792fb661"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['о', 'н', 'с', 'п', 'а', 'с', 'м', 'н', 'е', 'ж', 'и', 'з', 'н', 'ь', ',', 'и', 'я', 'с', 'м', 'о', 'т', 'р', 'е', 'л', 'н', 'а', 'н', 'е', 'г', 'о', 'с', 'н', 'и', 'з', 'у', 'в', 'в', 'е', 'р', 'х', 'с', 'с', 'а', 'м', 'о', 'г', 'о', 'д', 'е', 'т', 'с', 'т', 'в', 'а', '.', 'о', 'д', 'н', 'а', 'к', 'о', 'я', 'х', 'о', 'т', 'е', 'л', 'д', 'о', 'г', 'н', 'а', 'т', 'ь', 'е', 'г', 'о', 'е', 'щ', 'ё', 'б', 'о', 'л', 'ь', 'ш', 'е', ',', 'с', 'т', 'а', 'т', 'ь', 'д', 'л', 'я', 'н', 'е', 'г', 'о', 'е', 'щ', 'ё', 'б', 'о', 'л', 'е', 'е', 'с', 'и', 'л', 'ь', 'н', 'ы', 'м', 'ч', 'е', 'л', 'о', 'в', 'е', 'к', 'о', 'м', '.', 'х', 'о', 'т', 'я', ',', 'д', 'о', 'л', 'ж', 'н', 'о', 'б', 'ы', 'т', 'ь', ',', 'о', 'н', 'п', 'л', 'о', 'х', 'о', 'м', 'е', 'н', 'я', 'п', 'о', 'м', 'н', 'и', 'т', '.', 'м', 'ы', 'н', 'и', 'к', 'о', 'г', 'д', 'а', 'п', 'о', '-', 'н', 'а', 'с', 'т', 'о', 'я', 'щ', 'е', 'м', 'у', 'н', 'е', 'р', 'а', 'з', 'г', 'о', 'в', 'а', 'р', 'и', 'в', 'а', 'л', 'и', '.', 'я', 'х', 'о', 'ч', 'у', 'з', 'а', 'щ', 'и', 'т', 'и', 'т', 'ь', 'е', 'г', 'о', '.', 'о', 'н', 'п', 'р', 'и', 'с', 'т', 'а', 'л', 'ь', 'н', 'о', 'п', 'о', 'с', 'м', 'о', 'т', 'р', 'е', 'л', 'н', 'е', 'с', 'е', 'л', 'я', 'н', 'я', ':', '-', 'е', 'с', 'л', 'и', 'т', 'в', 'о', 'я', 'м', 'е', 'ч', 'т', 'а', '-', 'с', 'п', 'а', 'с', 'т', 'и', 'л', 'ю', 'д', 'е', 'й', ',', 'т', 'о', 'м', 'о', 'я', 'м', 'е', 'ч', 'т', 'а', '-', 'э', 'т', 'о', 'т', 'о', 'л', 'ь', 'к', 'о', 'т', 'ы', '.']\n",
            "['h', 'e', 's', 'a', 'v', 'e', 'm', 'y', 'l', 'i', 'f', 'e', ',', 'a', 'n', 'd', 'i', 'l', 'o', 'o', 'k', 'u', 'p', 't', 'o', 'h', 'i', 'm', 's', 'i', 'n', 'c', 'c', 'h', 'i', 'l', 'd', 'h', 'o', 'o', 'd', '.', 'h', 'o', 'w', 'e', 'v', ',', 'i', 'w', 'a', 'n', 't', 't', 'o', 'c', 'a', 't', 'c', 'h', 'u', 'p', 'w', 'i', 't', 'h', 'h', 'i', 'm', 'e', 'v', 'e', 'n', 'm', 'o', 'r', 'e', ',', 't', 'o', 'b', 'e', 'c', 'o', 'm', 'a', 'n', 'e', 'v', 'e', 'n', 's', 't', 'r', 'o', 'n', 'g', 'e', 'r', 'p', 'e', 'r', 's', 'o', 'n', 'f', 'o', 'r', 'h', 'i', 'm', '.', 'a', 'l', 't', 'h', 'o', 'u', 'g', 'h', ',', 'h', 'e', 'm', 'u', 's', 't', 'n', 'o', 't', 'r', 'e', 'm', 'e', 'm', 'b', 'm', 'e', 'w', 'e', 'l', 'l', '.', 'w', 'e', 'n', 'e', 'v', 'e', 'r', 'r', 'e', 'a', 'l', 'l', 'i', 't', 'a', 'l', 'k', '.', 'i', 'w', 'a', 'n', 't', 't', 'o', 'p', 'r', 'o', 't', 'e', 'c', 't', 'h', 'i', 'm', '.', 'h', 'e', 'l', 'o', 'o', 'k', 'c', 'l', 'o', 's', 'e', 'a', 't', 's', 'e', 'l', 'y', 'a', 'n', ':', '-', 'i', 'f', 'y', 'o', 'u', 'r', 'd', 'r', 'e', 'a', 'm', 'i', 's', 't', 'o', 's', 'a', 'v', 'e', 'p', 'e', 'o', 'p', 'l', ',', 't', 'h', 'e', 'n', 'm', 'y', 'd', 'r', 'e', 'a', 'm', 'i', 's', 'o', 'n', 'l', 'i', 'y', 'o', 'u', '.']\n"
          ]
        }
      ]
    }
  ]
}