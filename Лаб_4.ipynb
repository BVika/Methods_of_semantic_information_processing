{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtEH/MhoxBQ9x/cV81skcJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BVika/Methods_of_semantic_information_processing/blob/main/%D0%9B%D0%B0%D0%B1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задания\n",
        "Реализовать GPT как в п.2"
      ],
      "metadata": {
        "id": "JnP4Vb-kbe-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка библиотек"
      ],
      "metadata": {
        "id": "cL7g_C46bNcv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJSgvGRua20z",
        "outputId": "9189fb4a-e9b8-47db-814a-07350f247aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install pymorphy3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорт Библиотек и Загрузка Данных"
      ],
      "metadata": {
        "id": "TgcaeVFIi21H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzk7jmlgdPR2",
        "outputId": "b033c426-9228-4004-b565-6111d5978c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Текст(датабейс)"
      ],
      "metadata": {
        "id": "e_LgY0rYdSfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'Солнце медленно поднималось над горизонтом, окрашивая небо в яркие оттенки оранжевого и розового. Птицы начали петь, наполняя утро мелодиями.'\n",
        "\n",
        "phrases = [\n",
        "    'Солнце поднималось',\n",
        "    'Небо окрашивалось',\n",
        "    'Птицы начали',\n",
        "    'Утро наполнилось',\n",
        "    'Горизонт был',\n",
        "    'Мир просыпался',\n",
        "    'Звуки природы',\n",
        "    'Время летело'\n",
        "    ]\n",
        "\n",
        "mask = [\n",
        "    'горизонтом',\n",
        "    'в краски',\n",
        "    'петь',\n",
        "    'мелодиями',\n",
        "    'красивым',\n",
        "    'новыми',\n",
        "    'звучали',\n",
        "    'быстро'\n",
        "    ]"
      ],
      "metadata": {
        "id": "nyo7jgHKdS-G"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обработка текста"
      ],
      "metadata": {
        "id": "fh_fRDkGmLU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def split_text(self, content: str) -> list[str]:\n",
        "        \"\"\"Токенизация текста.\"\"\"\n",
        "        return word_tokenize(content)\n",
        "\n",
        "    def normalize_words(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Лемматизация токенов.\"\"\"\n",
        "        morph = MorphAnalyzer()\n",
        "        return [morph.parse(word)[0].normal_form for word in tokens]\n",
        "\n",
        "    def reduce_words(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Стемминг токенов.\"\"\"\n",
        "        stemmer = SnowballStemmer(\"russian\")\n",
        "        return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    def create_vector(self, tokens: list[str]) -> list[int]:\n",
        "        \"\"\"Создание векторного представления слов.\"\"\"\n",
        "        vector_dict = {}\n",
        "        vector_result = []\n",
        "        for word in tokens:\n",
        "            if word not in vector_dict:\n",
        "                vector_dict[word] = len(vector_dict)\n",
        "            vector_result.append(vector_dict[word])\n",
        "        return vector_result\n",
        "\n",
        "    def filter_stopwords(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Удаление стоп-слов из токенов.\"\"\"\n",
        "        stop_words = set(stopwords.words('russian')).union(['.', ',', ':', '?', '!'])\n",
        "        return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    def word_frequency(self, tokens: list[str]) -> dict[str, int]:\n",
        "        \"\"\"Создание словаря частот слов.\"\"\"\n",
        "        freq_dict = {}\n",
        "        for word in tokens:\n",
        "            freq_dict[word] = freq_dict.get(word, 0) + 1\n",
        "        return freq_dict\n",
        "\n",
        "    def term_frequency(self, tokens: list[str]) -> dict[str, float]:\n",
        "        \"\"\"Расчет частоты термина (TF).\"\"\"\n",
        "        freq_dict = self.word_frequency(tokens)\n",
        "        total_tokens = len(tokens)\n",
        "        return {word: count / total_tokens for word, count in freq_dict.items()}\n",
        "\n",
        "    def inverse_document_frequency(self, documents: list[list[str]]) -> dict[str, float]:\n",
        "        \"\"\"Расчет обратной частоты документа (IDF).\"\"\"\n",
        "        idf_dict = {}\n",
        "        all_words = set(word for doc in documents for word in set(doc))\n",
        "        total_docs = len(documents)\n",
        "        for word in all_words:\n",
        "            idf_dict[word] = math.log(total_docs / sum(word in doc for doc in documents))\n",
        "        return idf_dict\n",
        "\n",
        "    def tf_idf(self, documents: list[list[str]], doc_index: int) -> dict[str, float]:\n",
        "        \"\"\"Расчет TF-IDF для заданного документа.\"\"\"\n",
        "        tf = self.term_frequency(documents[doc_index])\n",
        "        idf = self.inverse_document_frequency(documents)\n",
        "        return {word: tf[word] * idf[word] for word in tf}\n"
      ],
      "metadata": {
        "id": "dMYaQnC9yYs4"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Нормализация\" весов"
      ],
      "metadata": {
        "id": "qAFowmvignSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_initialization(size):\n",
        "    limit = np.sqrt(6 / (size[0] + size[1]))\n",
        "    return np.random.uniform(-limit, limit, size)"
      ],
      "metadata": {
        "id": "UaCDATxWVhKg"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer:\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, learning_rate=0.01):\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"Инициализация параметров модели.\"\"\"\n",
        "        self.embeddings = np.random.randn(self.vocab_size, self.embed_size) * np.sqrt(1. / self.embed_size)\n",
        "        self.W_query = xavier_initialization((self.embed_size, self.hidden_size))\n",
        "        self.W_key = xavier_initialization((self.embed_size, self.hidden_size))\n",
        "        self.W_value = xavier_initialization((self.embed_size, self.hidden_size))\n",
        "        self.W_output = xavier_initialization((self.hidden_size, self.vocab_size))\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x))  # Для численной стабильности\n",
        "        return exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "    def forward_pass(self, input_indices):\n",
        "        \"\"\"Прямой проход через модель.\"\"\"\n",
        "        self.input_indices = input_indices\n",
        "        self.embedded_input = self.get_embedded_input(input_indices)\n",
        "\n",
        "        self.Q, self.K, self.V = self.calculate_attention_components()\n",
        "        self.attention_weights = self.calculate_attention_weights()\n",
        "        self.attended_values = self.calculate_attended_values()\n",
        "\n",
        "        self.context_vector = np.mean(self.attended_values, axis=0)\n",
        "        self.logits = self.calculate_logits()\n",
        "        self.probabilities = self.softmax(self.logits)\n",
        "\n",
        "        return self.probabilities\n",
        "\n",
        "    def get_embedded_input(self, input_indices):\n",
        "        \"\"\"Получение встраивания входных индексов.\"\"\"\n",
        "        return self.embeddings[input_indices]  # (seq_len, embed_size)\n",
        "\n",
        "    def calculate_attention_components(self):\n",
        "        \"\"\"Расчет Q, K и V.\"\"\"\n",
        "        Q = self.embedded_input @ self.W_query  # (seq_len, hidden_size)\n",
        "        K = self.embedded_input @ self.W_key\n",
        "        V = self.embedded_input @ self.W_value\n",
        "        return Q, K, V\n",
        "\n",
        "    def calculate_attention_weights(self):\n",
        "        \"\"\"Расчет весов внимания.\"\"\"\n",
        "        attention_scores = self.Q @ self.K.T / np.sqrt(self.embed_size)  # (seq_len, seq_len)\n",
        "        return self.softmax(attention_scores)  # (seq_len, seq_len)\n",
        "\n",
        "    def calculate_attended_values(self):\n",
        "        \"\"\"Расчет значений, на которые обращается внимание.\"\"\"\n",
        "        return self.attention_weights @ self.V  # (seq_len, hidden_size)\n",
        "\n",
        "    def calculate_logits(self):\n",
        "        \"\"\"Расчет логитов для выходного слоя.\"\"\"\n",
        "        return self.context_vector @ self.W_output  # (vocab_size,)\n",
        "\n",
        "    def backward_pass(self, target_index):\n",
        "        \"\"\"Обратный проход для обновления весов.\"\"\"\n",
        "        d_logits = self.probabilities.copy()\n",
        "        d_logits[target_index] -= 1  # (vocab_size,)\n",
        "\n",
        "        dW_output = self.calculate_dW_output(d_logits)\n",
        "        d_context = d_logits @ self.W_output.T  # (hidden_size,)\n",
        "\n",
        "        d_attended_values = self.calculate_d_attended_values(d_context)\n",
        "        dV = self.attention_weights.T @ d_attended_values  # (hidden_size,)\n",
        "\n",
        "        d_attention_weights = self.calculate_d_attention_weights(d_attended_values)\n",
        "        d_scores = self.calculate_d_scores(d_attention_weights)\n",
        "\n",
        "        dQ, dK = self.calculate_gradients(d_scores)\n",
        "        dW_query, dW_key, dW_value = self.calculate_weight_gradients(dQ, dK, dV)\n",
        "\n",
        "        self.update_embeddings(dQ, dK, dV)\n",
        "        self.update_weights(dW_output, dW_query, dW_key, dW_value)\n",
        "\n",
        "    def calculate_dW_output(self, d_logits):\n",
        "        \"\"\"Расчет градиента для выходных весов.\"\"\"\n",
        "        return np.outer(self.context_vector, d_logits)  # (hidden_size, vocab_size)\n",
        "\n",
        "    def calculate_d_attended_values(self, d_context):\n",
        "        \"\"\"Расчет градиента для значений, на которые обращается внимание.\"\"\"\n",
        "        # d_context имеет размер (hidden_size,)\n",
        "        # Учитываем, что d_attended_values - это среднее значение, поэтому делим на количество последовательных элементов\n",
        "        seq_len = self.attended_values.shape[0]\n",
        "        return np.ones_like(self.attended_values) * (d_context / seq_len)\n",
        "\n",
        "    def calculate_d_attention_weights(self, d_attended_values):\n",
        "        \"\"\"Расчет градиента для весов внимания.\"\"\"\n",
        "        # d_attended_values имеет размер (seq_len, hidden_size)\n",
        "        return d_attended_values @ self.V.T  # (seq_len, seq_len)\n",
        "\n",
        "    def calculate_d_scores(self, d_attention_weights):\n",
        "        \"\"\"Расчет градиента для оценок внимания.\"\"\"\n",
        "        # Используем производную функции softmax\n",
        "        return d_attention_weights * self.attention_weights * (1 - self.attention_weights)  # (seq_len, seq_len)\n",
        "\n",
        "    def calculate_gradients(self, d_scores):\n",
        "        \"\"\"Расчет градиентов для Q и K.\"\"\"\n",
        "        dQ = d_scores @ self.K / np.sqrt(self.embed_size)  # (seq_len, hidden_size)\n",
        "        dK = d_scores.T @ self.Q / np.sqrt(self.embed_size)  # (hidden_size, seq_len)\n",
        "        return dQ, dK\n",
        "\n",
        "    def calculate_weight_gradients(self, dQ, dK, dV):\n",
        "        \"\"\"Расчет градиентов для весов Q, K и V.\"\"\"\n",
        "        dW_query = self.embedded_input.T @ dQ  # (embed_size, hidden_size)\n",
        "        dW_key = self.embedded_input.T @ dK  # (embed_size, hidden_size)\n",
        "        dW_value = self.embedded_input.T @ dV  # (embed_size, hidden_size)\n",
        "        return dW_query, dW_key, dW_value\n",
        "\n",
        "    def update_embeddings(self, dQ, dK, dV):\n",
        "        \"\"\"Обновление встраиваний на основе градиентов.\"\"\"\n",
        "        d_embedding = dQ @ self.W_query.T + dK @ self.W_key.T + dV @ self.W_value.T  # (seq_len, embed_size)\n",
        "        for i, idx in enumerate(self.input_indices):\n",
        "            self.embeddings[idx] -= self.learning_rate * d_embedding[i]\n",
        "\n",
        "    def update_weights(self, dW_output, dW_query, dW_key, dW_value):\n",
        "        \"\"\"Обновление весов модели.\"\"\"\n",
        "        self.W_output -= self.learning_rate * dW_output\n",
        "        self.W_query -= self.learning_rate * dW_query\n",
        "        self.W_key -= self.learning_rate * dW_key\n",
        "        self.W_value -= self.learning_rate * dW_value\n",
        "\n",
        "    def train_step(self, input_indices, target_index):\n",
        "        \"\"\"Шаг обучения.\"\"\"\n",
        "        self.forward_pass(input_indices)\n",
        "        self.backward_pass(target_index)\n",
        "\n",
        "    def predict(self, input_indices):\n",
        "        \"\"\"Предсказание на основе входных индексов.\"\"\"\n",
        "        probabilities = self.forward_pass(input_indices)\n",
        "        return np.argmax(probabilities)"
      ],
      "metadata": {
        "id": "yPkAAD1TG_8W"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обработка текста и подготовка данных"
      ],
      "metadata": {
        "id": "oijOF-LVhW6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_processor = TextProcessor()"
      ],
      "metadata": {
        "id": "WCNLRBlGHVTy"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обработка текстов\n",
        "tokens = text_processor.filter_stopwords(text_processor.normalize_words(text_processor.split_text(input_text)))\n",
        "# Векторизация словаря\n",
        "vocabulary = list(set(tokens))\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "print(tokens)\n",
        "print(vocabulary)\n",
        "print(word_to_index)\n",
        "print(index_to_word)"
      ],
      "metadata": {
        "id": "XUedFTpBHVed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c50b99-7a3b-4557-be53-f54e39064bdd"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['солнце', 'медленно', 'подниматься', 'горизонт', 'окрашивать', 'небо', 'яркий', 'оттенок', 'оранжевый', 'розовый', 'птица', 'начать', 'петь', 'наполнять', 'утро', 'мелодия']\n",
            "['солнце', 'птица', 'яркий', 'окрашивать', 'утро', 'мелодия', 'горизонт', 'медленно', 'небо', 'подниматься', 'наполнять', 'начать', 'розовый', 'оранжевый', 'оттенок', 'петь']\n",
            "{'солнце': 0, 'птица': 1, 'яркий': 2, 'окрашивать': 3, 'утро': 4, 'мелодия': 5, 'горизонт': 6, 'медленно': 7, 'небо': 8, 'подниматься': 9, 'наполнять': 10, 'начать': 11, 'розовый': 12, 'оранжевый': 13, 'оттенок': 14, 'петь': 15}\n",
            "{0: 'солнце', 1: 'птица', 2: 'яркий', 3: 'окрашивать', 4: 'утро', 5: 'мелодия', 6: 'горизонт', 7: 'медленно', 8: 'небо', 9: 'подниматься', 10: 'наполнять', 11: 'начать', 12: 'розовый', 13: 'оранжевый', 14: 'оттенок', 15: 'петь'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка данных для обучения"
      ],
      "metadata": {
        "id": "ak_7lMb-hl_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for phrase, label in zip(phrases, mask):\n",
        "    print(f\"Обрабатываемая фраза: {phrase}, метка: {label}\")  # Выводим фразы и метки\n",
        "    tokens = text_processor.filter_stopwords(text_processor.normalize_words(text_processor.split_text(phrase)))\n",
        "    print(f\"Токены после фильтрации: {tokens}\")  # Выводим токены после фильтрации\n",
        "\n",
        "    if len(tokens) < 2:\n",
        "        print(\"Пропускаем фразу из-за недостаточной длины.\")  # Сообщение о пропуске\n",
        "        continue  # Пропускаем фразы с недостаточным количеством токенов\n",
        "\n",
        "    input_indices = [word_to_index.get(word, -1) for word in tokens if word in word_to_index]\n",
        "    input_indices = [index for index in input_indices if index != -1]  # Удаляем недопустимые индексы\n",
        "\n",
        "    if len(input_indices) == 0:\n",
        "        print(\"Пропускаем фразу из-за отсутствия допустимых токенов.\")  # Сообщение о пропуске\n",
        "        continue  # Пропускаем пустые массивы\n",
        "\n",
        "    label_index = word_to_index.get(label, -1)\n",
        "    if label_index == -1:\n",
        "        print(\"Пропускаем фразу из-за недопустимой метки.\")  # Сообщение о пропуске\n",
        "        continue  # Пропускаем, если метка недопустима\n",
        "\n",
        "    X.append(input_indices)\n",
        "    y.append(label_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYCyOTG9HVgv",
        "outputId": "7bac70e9-b47a-4de8-b5db-f9c5136d8b16"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обрабатываемая фраза: Солнце поднималось, метка: горизонтом\n",
            "Токены после фильтрации: ['солнце', 'подниматься']\n",
            "Пропускаем фразу из-за недопустимой метки.\n",
            "Обрабатываемая фраза: Небо окрашивалось, метка: в краски\n",
            "Токены после фильтрации: ['небо', 'окрашиваться']\n",
            "Пропускаем фразу из-за недопустимой метки.\n",
            "Обрабатываемая фраза: Птицы начали, метка: петь\n",
            "Токены после фильтрации: ['птица', 'начать']\n",
            "Обрабатываемая фраза: Утро наполнилось, метка: мелодиями\n",
            "Токены после фильтрации: ['утро', 'наполниться']\n",
            "Пропускаем фразу из-за недопустимой метки.\n",
            "Обрабатываемая фраза: Горизонт был, метка: красивым\n",
            "Токены после фильтрации: ['горизонт']\n",
            "Пропускаем фразу из-за недостаточной длины.\n",
            "Обрабатываемая фраза: Мир просыпался, метка: новыми\n",
            "Токены после фильтрации: ['мир', 'просыпаться']\n",
            "Пропускаем фразу из-за отсутствия допустимых токенов.\n",
            "Обрабатываемая фраза: Звуки природы, метка: звучали\n",
            "Токены после фильтрации: ['звук', 'природа']\n",
            "Пропускаем фразу из-за отсутствия допустимых токенов.\n",
            "Обрабатываемая фраза: Время летело, метка: быстро\n",
            "Токены после фильтрации: ['время', 'лететь']\n",
            "Пропускаем фразу из-за отсутствия допустимых токенов.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение модели"
      ],
      "metadata": {
        "id": "5b2KrsofhydC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0DtP-xO9hxdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(vocab_size=len(vocabulary), embed_size=16, hidden_size=32)\n",
        "\n",
        "for epoch in range(10000):\n",
        "    total_loss = 0\n",
        "    for i in range(len(X)):\n",
        "        model.train_step(X[i], y[i])\n",
        "        probs = model.forward_pass(X[i])\n",
        "\n",
        "        # Проверяем, чтобы не было нуля\n",
        "        if probs[y[i]] > 0:\n",
        "            total_loss += -np.log(probs[y[i]])\n",
        "        else:\n",
        "            total_loss += np.inf  # Или какое-то значение, чтобы указать на ошибку\n",
        "\n",
        "    # Вывод информации о процессе обучения\n",
        "    if epoch % 1000 == 0:\n",
        "        avg_loss = total_loss / len(X)\n",
        "        print(f\"Epoch {epoch}: Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKGGWYi_HVkD",
        "outputId": "6bcf5ba7-eb65-4688-822f-5e9a0598bae6"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss: 2.7791\n",
            "Epoch 1000: Loss: 0.0026\n",
            "Epoch 2000: Loss: 0.0010\n",
            "Epoch 3000: Loss: 0.0006\n",
            "Epoch 4000: Loss: 0.0004\n",
            "Epoch 5000: Loss: 0.0003\n",
            "Epoch 6000: Loss: 0.0002\n",
            "Epoch 7000: Loss: 0.0002\n",
            "Epoch 8000: Loss: 0.0002\n",
            "Epoch 9000: Loss: 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предсказание"
      ],
      "metadata": {
        "id": "Q2StoTSjiAxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(\"\\nПредсказания для новых фраз:\")\n",
        "    new_phrases = [\n",
        "        'Солнце поднималось над горизонтом',\n",
        "        'Небо стало ярким',\n",
        "        'Птицы пели мелодии',\n",
        "        'Утро было прекрасным'\n",
        "    ]\n",
        "\n",
        "    for phrase in new_phrases:\n",
        "        print(f\"\\nОбрабатываемая новая фраза: '{phrase}'\")\n",
        "        tokens = text_processor.filter_stopwords(text_processor.normalize_words(text_processor.split_text(phrase)))\n",
        "        print(f\"Токены после фильтрации: {tokens}\")\n",
        "\n",
        "        input_indices = [word_to_index.get(word, -1) for word in tokens if word in word_to_index]\n",
        "        input_indices = [index for index in input_indices if index != -1]  # Удаляем недопустимые индексы\n",
        "\n",
        "        if len(input_indices) == 0:\n",
        "            print(\"Нет допустимых токенов для предсказания.\")\n",
        "            continue\n",
        "\n",
        "        predicted_index = model.predict(input_indices)\n",
        "        predicted_word = index_to_word.get(predicted_index, \"Неизвестное слово\")\n",
        "        print(f\"Предсказанное слово: '{predicted_word}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MtLjQQCHVmj",
        "outputId": "359e579b-ddf3-4611-89c4-447d5785c787"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Предсказания для новых фраз:\n",
            "\n",
            "Обрабатываемая новая фраза: 'Солнце поднималось над горизонтом'\n",
            "Токены после фильтрации: ['солнце', 'подниматься', 'горизонт']\n",
            "Предсказанное слово: 'петь'\n",
            "\n",
            "Обрабатываемая новая фраза: 'Небо стало ярким'\n",
            "Токены после фильтрации: ['небо', 'стать', 'яркий']\n",
            "Предсказанное слово: 'петь'\n",
            "\n",
            "Обрабатываемая новая фраза: 'Птицы пели мелодии'\n",
            "Токены после фильтрации: ['птица', 'петь', 'мелодия']\n",
            "Предсказанное слово: 'петь'\n",
            "\n",
            "Обрабатываемая новая фраза: 'Утро было прекрасным'\n",
            "Токены после фильтрации: ['утро', 'прекрасный']\n",
            "Предсказанное слово: 'наполнять'\n"
          ]
        }
      ]
    }
  ]
}