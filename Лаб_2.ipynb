{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPPlmR5Rl4k2TXJAly2VBP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BVika/Methods_of_semantic_information_processing/blob/main/%D0%9B%D0%B0%D0%B1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задания"
      ],
      "metadata": {
        "id": "ZfTW7XPOKczy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Применить к текстам лемматизацию, удаление стоп слов и токенизацию по словам\n",
        "2. Реализовать Bag of Words\n",
        "3. Реализовать TF-IDF"
      ],
      "metadata": {
        "id": "r1WG8s4gKj9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка библиотек"
      ],
      "metadata": {
        "id": "vJePciSrLs0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wXDByHbLtGK",
        "outputId": "360eb54d-252e-43ca-85f2-c6058bf4e173"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE703c8vOE2q",
        "outputId": "f4f7291a-5a78-4b02-8787-8d83b2d09630"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Задание 1**\n",
        "*Применить к текстам лемматизацию, удаление стоп слов и токенизацию по словам*"
      ],
      "metadata": {
        "id": "hHZ-6jDSLA3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `на русском`\n"
      ],
      "metadata": {
        "id": "tq6y4FS3LXkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт библиотек"
      ],
      "metadata": {
        "id": "blqbYGpqOaZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymystem3 import Mystem\n",
        "from typing import List\n",
        "import pymorphy3"
      ],
      "metadata": {
        "id": "eE_27w6sOeji"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример текста\n",
        "texts = [\n",
        "    \"Я люблю программирование и изучение новых технологий.\",\n",
        "    \"Машинное обучение - это увлекательная область.\"\n",
        "]"
      ],
      "metadata": {
        "id": "0Ms1Km7wvUrj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG3fjcECKPDF",
        "outputId": "f06cd2df-a010-4c2f-de08-096432f0ba0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        }
      ],
      "source": [
        "# Загрузка необходимых ресурсов\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "m = Mystem()  # Программа MyStem производит морфологический анализ текста на русском языке\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize(text: List[str]) -> List[str]:\n",
        "    lemmas = []\n",
        "    for word in text:\n",
        "        lemmas.extend(m.lemmatize(word))  # Лемматизируем каждое слово\n",
        "        cleaned_lemmas = [lemma.strip() for lemma in lemmas if lemma.strip() and lemma not in [' ']]\n",
        "    return cleaned_lemmas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление стоп-слов и лемматизация для каждого текста\n",
        "for text in texts:\n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text)\n",
        "    # Удаление стоп-слов\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Лемматизация\n",
        "    lemmas = lemmatize(filtered_tokens)\n",
        "\n",
        "    # Результат\n",
        "    print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EVUCIq1vZNT",
        "outputId": "5e04806f-7896-4c93-bd9d-bdf2147e223e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['любить', 'программирование', 'изучение', 'новый', 'технология', '.']\n",
            "['машинный', 'обучение', '-', 'это', 'увлекательный', 'область', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `на английском`\n",
        "\n"
      ],
      "metadata": {
        "id": "O01rmpW7WCzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт библиотек"
      ],
      "metadata": {
        "id": "uQy9XYCqSvQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "X4ZNXVKtSy4p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример текста\n",
        "texts = [\n",
        "    \"I love programming and learning new technologies.\",\n",
        "    \"Machine learning is an exciting field.\"\n",
        "]"
      ],
      "metadata": {
        "id": "JCXKiDaOvRlc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка необходимых ресурсов\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Инициализация лемматизатора\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize(text: List[str]) -> List[str]:\n",
        "    lemmas = [lemmatizer.lemmatize(word) for word in text]  # Лемматизируем каждое слово\n",
        "    return lemmas\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqK9xMBaSvmR",
        "outputId": "f19998d7-793c-47dd-b99f-cd1e6d7fd1f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление стоп-слов и лемматизация для каждого текста\n",
        "for text in texts:\n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text)\n",
        "    # Удаление стоп-слов\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Лемматизация\n",
        "    lemmas = lemmatize(filtered_tokens)\n",
        "\n",
        "    # Результат\n",
        "    print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "805SqZEcvcED",
        "outputId": "c8206f67-5fb1-4225-970a-ead3a77da501"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'programming', 'learning', 'new', 'technology', '.']\n",
            "['Machine', 'learning', 'exciting', 'field', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Задание 2**\n",
        "Реализовать Bag of Words"
      ],
      "metadata": {
        "id": "nmTtkRKJWie5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "s5pcpTnPWxrY"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"Природа полна удивительных чудес, и в каждом чуде природы скрыта своя красота.\",\n",
        "    \"Леса и горы дарят нам ощущение спокойствия и гармонии, которые можно найти только в природе.\",\n",
        "    \"Птицы поют свои мелодии, и их мелодии наполняют воздух радостью и счастьем.\",\n",
        "    \"Река текет, отражая солнечные лучи и зелень вокруг, создавая волшебный мир природы.\",\n",
        "    \"Каждый сезон приносит свои краски и запахи, и каждый сезон природы уникален.\",\n",
        "    \"Цветы распускаются весной, радуя глаз яркими оттенками, и весна пробуждает природу.\",\n",
        "    \"Закат над озером создает волшебную атмосферу и умиротворение, которое ощущается в природе.\",\n",
        "    \"Прогулка по лесу позволяет ощутить силу и красоту природы, которая окружает нас повсюду.\"\n",
        "]"
      ],
      "metadata": {
        "id": "BevsUvQUvzqz"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для лемматизации\n",
        "def lemmatize(text: List[str]) -> List[str]:\n",
        "    lemmas = [lemmatizer.lemmatize(word) for word in text]  # Лемматизируем каждое слово\n",
        "    return lemmas"
      ],
      "metadata": {
        "id": "YKHmuYDoSRo9"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(texts: List[str]) -> List[List[str]]:\n",
        "\n",
        "    \"\"\"Предобработка текста, удаляющая стоп-слова и знаки препинания, возвращающая список списков токенов.\"\"\"\n",
        "    punctuation_marks = ['!', ',', '(', ')', ':', '-', '?', '.', '..', '...', '«', '»', ';', '–', '--']\n",
        "    stop_words = set(stopwords.words(\"russian\"))\n",
        "    tokenized_texts = []\n",
        "\n",
        "    for text in texts:\n",
        "\n",
        "        text = text.lower() # уриводим текст к нижнему регистру\n",
        "        # удаляем знаки препинания\n",
        "        for punctuation in punctuation_marks:\n",
        "            text = text.replace(punctuation, '')\n",
        "        tokens = text.split()# токенизируем текст\n",
        "\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words] # удаляем стоп-слова\n",
        "\n",
        "        lemmatized_tokens = lemmatize(filtered_tokens) #лемматизация\n",
        "\n",
        "        tokenized_texts.append(filtered_tokens)\n",
        "    return tokenized_texts\n",
        "\n",
        "\n",
        "# токинизация и лемматизация\n",
        "tokenized_texts = preprocessing(texts)\n",
        "for row in tokenized_texts:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIwfc8NhyEUr",
        "outputId": "d633af9b-4393-4cc7-a759-6c9100121a7d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['природа', 'полна', 'удивительных', 'чудес', 'каждом', 'чуде', 'природы', 'скрыта', 'своя', 'красота']\n",
            "['леса', 'горы', 'дарят', 'нам', 'ощущение', 'спокойствия', 'гармонии', 'которые', 'найти', 'природе']\n",
            "['птицы', 'поют', 'свои', 'мелодии', 'мелодии', 'наполняют', 'воздух', 'радостью', 'счастьем']\n",
            "['река', 'текет', 'отражая', 'солнечные', 'лучи', 'зелень', 'вокруг', 'создавая', 'волшебный', 'мир', 'природы']\n",
            "['каждый', 'сезон', 'приносит', 'свои', 'краски', 'запахи', 'каждый', 'сезон', 'природы', 'уникален']\n",
            "['цветы', 'распускаются', 'весной', 'радуя', 'глаз', 'яркими', 'оттенками', 'весна', 'пробуждает', 'природу']\n",
            "['закат', 'озером', 'создает', 'волшебную', 'атмосферу', 'умиротворение', 'которое', 'ощущается', 'природе']\n",
            "['прогулка', 'лесу', 'позволяет', 'ощутить', 'силу', 'красоту', 'природы', 'которая', 'окружает', 'повсюду']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict(texts: list[list[str]]) -> dict[str, int]:\n",
        "\n",
        "    \"\"\"Создание словаря уникальных слов из текстов.\"\"\"\n",
        "    unique_words = set()\n",
        "    for text in texts:\n",
        "        unique_words.update(text)  #добавление уникальных слов в множество\n",
        "\n",
        "    return {word: index for index, word in enumerate(unique_words)}  #присвоение индексов\n",
        "\n",
        "# создание словаря уникальных слов\n",
        "word_dict = make_dict(tokenized_texts)\n",
        "print(word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8wN6bvKTYel",
        "outputId": "9220b332-ac32-4ee8-c121-7e86635baf62"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'которое': 0, 'мелодии': 1, 'счастьем': 2, 'нам': 3, 'ощущение': 4, 'красота': 5, 'глаз': 6, 'отражая': 7, 'свои': 8, 'ощутить': 9, 'каждом': 10, 'птицы': 11, 'поют': 12, 'уникален': 13, 'горы': 14, 'пробуждает': 15, 'вокруг': 16, 'природе': 17, 'весна': 18, 'запахи': 19, 'прогулка': 20, 'спокойствия': 21, 'атмосферу': 22, 'полна': 23, 'природа': 24, 'волшебную': 25, 'красоту': 26, 'найти': 27, 'каждый': 28, 'удивительных': 29, 'гармонии': 30, 'лучи': 31, 'чуде': 32, 'яркими': 33, 'озером': 34, 'приносит': 35, 'мир': 36, 'река': 37, 'радостью': 38, 'наполняют': 39, 'солнечные': 40, 'краски': 41, 'силу': 42, 'природы': 43, 'скрыта': 44, 'которая': 45, 'ощущается': 46, 'леса': 47, 'оттенками': 48, 'умиротворение': 49, 'чудес': 50, 'текет': 51, 'создавая': 52, 'сезон': 53, 'создает': 54, 'закат': 55, 'волшебный': 56, 'зелень': 57, 'повсюду': 58, 'весной': 59, 'цветы': 60, 'природу': 61, 'лесу': 62, 'своя': 63, 'которые': 64, 'окружает': 65, 'позволяет': 66, 'радуя': 67, 'распускаются': 68, 'воздух': 69, 'дарят': 70}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_num_words(texts: list[list[str]], word_dict: dict[str, int]) -> list[list[int]]:\n",
        "\n",
        "    \"\"\"Подсчет количества вхождений слов для каждого текста.\"\"\"\n",
        "    word_counts = []\n",
        "    for text in texts:\n",
        "        count_vector = [0] * len(word_dict)  # инициализация вектора счетчиков\n",
        "        for word in text:\n",
        "            if word in word_dict:\n",
        "                count_vector[word_dict[word]] += 1\n",
        "        word_counts.append(count_vector)\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "# подсчет количества слов\n",
        "word_counts = count_num_words(tokenized_texts, word_dict)\n",
        "for row in word_counts:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCFxN3JtTh0f",
        "outputId": "4764fae3-901e-4cac-89e5-73e86a35465d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "[0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Словарь уникальных слов:\", word_dict)\n",
        "print(\"Матрица Bag of Words:\")\n",
        "for row in word_counts:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AgNRk-ETjYU",
        "outputId": "4cbef6dd-4f0f-4edc-a97c-474603c684dd"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь уникальных слов: {'которое': 0, 'мелодии': 1, 'счастьем': 2, 'нам': 3, 'ощущение': 4, 'красота': 5, 'глаз': 6, 'отражая': 7, 'свои': 8, 'ощутить': 9, 'каждом': 10, 'птицы': 11, 'поют': 12, 'уникален': 13, 'горы': 14, 'пробуждает': 15, 'вокруг': 16, 'природе': 17, 'весна': 18, 'запахи': 19, 'прогулка': 20, 'спокойствия': 21, 'атмосферу': 22, 'полна': 23, 'природа': 24, 'волшебную': 25, 'красоту': 26, 'найти': 27, 'каждый': 28, 'удивительных': 29, 'гармонии': 30, 'лучи': 31, 'чуде': 32, 'яркими': 33, 'озером': 34, 'приносит': 35, 'мир': 36, 'река': 37, 'радостью': 38, 'наполняют': 39, 'солнечные': 40, 'краски': 41, 'силу': 42, 'природы': 43, 'скрыта': 44, 'которая': 45, 'ощущается': 46, 'леса': 47, 'оттенками': 48, 'умиротворение': 49, 'чудес': 50, 'текет': 51, 'создавая': 52, 'сезон': 53, 'создает': 54, 'закат': 55, 'волшебный': 56, 'зелень': 57, 'повсюду': 58, 'весной': 59, 'цветы': 60, 'природу': 61, 'лесу': 62, 'своя': 63, 'которые': 64, 'окружает': 65, 'позволяет': 66, 'радуя': 67, 'распускаются': 68, 'воздух': 69, 'дарят': 70}\n",
            "Матрица Bag of Words:\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "[0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Задание 3**\n",
        "Реализовать TF-IDF"
      ],
      "metadata": {
        "id": "cfTmQFQjv4sL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Частотность термина (TF). Измеряет, насколько часто слово встречается в документе. Рассчитывается как отношение количества вхождений слова к количеству слов в документе.\n",
        "\n",
        "Обратная частотность документа (IDF). Измеряет, насколько редким является слово в наборе документов. Рассчитывается как логарифм отношения количества документов в коллекции к количеству документов в коллекции, в которых встречается заданное слово.\n",
        "\n",
        "Общий показатель TF-IDF является произведением TF и IDF. Формула: TF-IDF = TF * log(IDF)."
      ],
      "metadata": {
        "id": "RcaeCYS9CavF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import List, Dict\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "q6qg1OuoH8kq"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(texts: List[str]) -> List[List[str]]:\n",
        "\n",
        "    \"\"\"Предобработка текста, удаляющая стоп-слова и знаки препинания, возвращающая список списков токенов.\"\"\"\n",
        "    punctuation_marks = ['!', ',', '(', ')', ':', '-', '?', '.', '..', '...', '«', '»', ';', '–', '--']\n",
        "    stop_words = set(stopwords.words(\"russian\"))\n",
        "    tokenized_texts = []\n",
        "\n",
        "    for text in texts:\n",
        "        text = text.lower() # приводим текст к нижнему регистру\n",
        "        # удаляем знаки препинания\n",
        "        for punctuation in punctuation_marks:\n",
        "            text = text.replace(punctuation, '')\n",
        "\n",
        "        tokens = text.split()# токенизируем текст\n",
        "\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words] # удаляем стоп-слова\n",
        "\n",
        "        lemmatized_tokens = lemmatize(filtered_tokens) #лемматизация\n",
        "\n",
        "        tokenized_texts.append(filtered_tokens)\n",
        "    return tokenized_texts\n",
        "\n",
        "\n",
        "tokenized_texts = preprocessing(texts)\n",
        "for row in tokenized_texts:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kjUGUDhzs2j",
        "outputId": "2074b6c4-9aec-4a1b-b21e-f0ee6585ff7a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['природа', 'полна', 'удивительных', 'чудес', 'каждом', 'чуде', 'природы', 'скрыта', 'своя', 'красота']\n",
            "['леса', 'горы', 'дарят', 'нам', 'ощущение', 'спокойствия', 'гармонии', 'которые', 'найти', 'природе']\n",
            "['птицы', 'поют', 'свои', 'мелодии', 'мелодии', 'наполняют', 'воздух', 'радостью', 'счастьем']\n",
            "['река', 'текет', 'отражая', 'солнечные', 'лучи', 'зелень', 'вокруг', 'создавая', 'волшебный', 'мир', 'природы']\n",
            "['каждый', 'сезон', 'приносит', 'свои', 'краски', 'запахи', 'каждый', 'сезон', 'природы', 'уникален']\n",
            "['цветы', 'распускаются', 'весной', 'радуя', 'глаз', 'яркими', 'оттенками', 'весна', 'пробуждает', 'природу']\n",
            "['закат', 'озером', 'создает', 'волшебную', 'атмосферу', 'умиротворение', 'которое', 'ощущается', 'природе']\n",
            "['прогулка', 'лесу', 'позволяет', 'ощутить', 'силу', 'красоту', 'природы', 'которая', 'окружает', 'повсюду']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict(texts: List[List[str]]) -> Dict[str, int]:\n",
        "\n",
        "    \"\"\"Создание словаря уникальных слов из текстов.\"\"\"\n",
        "    unique_words = set()\n",
        "    for text in texts:\n",
        "        unique_words.update(text)\n",
        "\n",
        "    return {word: index for index, word in enumerate(unique_words)}\n",
        "\n",
        "# создание словаря уникальных слов\n",
        "word_dict = make_dict(tokenized_texts)\n",
        "print(word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDe285MzGHgm",
        "outputId": "99746bac-02d4-416d-97f8-c6aee893d8cd"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'которое': 0, 'мелодии': 1, 'счастьем': 2, 'нам': 3, 'ощущение': 4, 'красота': 5, 'глаз': 6, 'отражая': 7, 'свои': 8, 'ощутить': 9, 'каждом': 10, 'птицы': 11, 'поют': 12, 'уникален': 13, 'горы': 14, 'пробуждает': 15, 'вокруг': 16, 'природе': 17, 'весна': 18, 'запахи': 19, 'прогулка': 20, 'спокойствия': 21, 'атмосферу': 22, 'полна': 23, 'природа': 24, 'волшебную': 25, 'красоту': 26, 'найти': 27, 'каждый': 28, 'удивительных': 29, 'гармонии': 30, 'лучи': 31, 'чуде': 32, 'яркими': 33, 'озером': 34, 'приносит': 35, 'мир': 36, 'река': 37, 'радостью': 38, 'наполняют': 39, 'солнечные': 40, 'краски': 41, 'силу': 42, 'природы': 43, 'скрыта': 44, 'которая': 45, 'ощущается': 46, 'леса': 47, 'оттенками': 48, 'умиротворение': 49, 'чудес': 50, 'текет': 51, 'создавая': 52, 'сезон': 53, 'создает': 54, 'закат': 55, 'волшебный': 56, 'зелень': 57, 'повсюду': 58, 'весной': 59, 'цветы': 60, 'природу': 61, 'лесу': 62, 'своя': 63, 'которые': 64, 'окружает': 65, 'позволяет': 66, 'радуя': 67, 'распускаются': 68, 'воздух': 69, 'дарят': 70}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tf(texts: List[List[str]], word_dict: Dict[str, int]) -> List[List[float]]:\n",
        "\n",
        "    \"\"\"Подсчет TF для каждого текста.\"\"\"\n",
        "    tf_matrix = []\n",
        "    for text in texts:\n",
        "        count_vector = [0] * len(word_dict)\n",
        "        for word in text:\n",
        "            if word in word_dict:\n",
        "                count_vector[word_dict[word]] += 1\n",
        "\n",
        "\n",
        "        total_words = len(text)\n",
        "        tf_vector = [count / total_words for count in count_vector]\n",
        "        tf_matrix.append(tf_vector)\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "\n",
        "tf_matrix = compute_tf(tokenized_texts, word_dict)\n",
        "for row in tf_matrix:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5ceJ-TpFoEs",
        "outputId": "99fc18e1-1002-4fbb-bbdd-0cff1885694f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1]\n",
            "[0.0, 0.2222222222222222, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.1111111111111111, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.09090909090909091, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.09090909090909091, 0.0, 0.0, 0.0, 0.09090909090909091, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0]\n",
            "[0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_idf(texts: List[List[str]], word_dict: Dict[str, int]) -> List[float]:\n",
        "\n",
        "    \"\"\"Подсчет IDF для слов.\"\"\"\n",
        "    idf_vector = []\n",
        "    num_docs = len(texts)\n",
        "\n",
        "    for word in word_dict.keys():\n",
        "        count = sum(1 for text in texts if word in text) #возвращает количество документов, в которых присутствует данное слово. Если слово не встречается ни в одном документе, count будет равно 0.\n",
        "        idf = math.log(num_docs / (count + 1))  # добавляем 1 для избежания деления на ноль\n",
        "        idf_vector.append(idf)\n",
        "\n",
        "    return idf_vector\n",
        "\n",
        "\n",
        "\n",
        "idf_vector = compute_idf(tokenized_texts, word_dict)\n",
        "print(idf_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4L9F1SGFtOm",
        "outputId": "e5870133-7f42-4b6f-ad4e-028ed231fdd8"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 0.9808292530117262, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 0.9808292530117262, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 0.47000362924573563, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(tf_matrix: List[List[float]], idf_vector: List[float]) -> List[List[float]]:\n",
        "\n",
        "    \"\"\"Подсчет TF-IDF на основе TF и IDF.\"\"\"\n",
        "    tfidf_matrix = []\n",
        "\n",
        "    for tf_vector in tf_matrix:\n",
        "        tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n",
        "        tfidf_matrix.append(tfidf_vector)\n",
        "    return tfidf_matrix\n",
        "\n",
        "\n",
        "tfidf_matrix = compute_tfidf(tf_matrix, idf_vector)\n",
        "for row in tfidf_matrix:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZMuJoMcFvsG",
        "outputId": "d1233041-c9b1-4359-9e98-062eb2acf28d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04700036292457357, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.09808292530117263, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905]\n",
            "[0.0, 0.3080654135821979, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10898102811241402, 0.0, 0.0, 0.15403270679109896, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.12602676010180824, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.04272760265870324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.12602676010180824, 0.0, 0.0, 0.0, 0.12602676010180824, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09808292530117263, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.04700036292457357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0]\n",
            "[0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10898102811241402, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.04700036292457357, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Словарь уникальных слов:\", word_dict)\n",
        "print(\"Матрица TF-IDF:\")\n",
        "for row in tfidf_matrix:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NFI4JJBH2Ma",
        "outputId": "0fb7a83b-0db5-43ff-e066-7be98fbf9ed7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь уникальных слов: {'которое': 0, 'мелодии': 1, 'счастьем': 2, 'нам': 3, 'ощущение': 4, 'красота': 5, 'глаз': 6, 'отражая': 7, 'свои': 8, 'ощутить': 9, 'каждом': 10, 'птицы': 11, 'поют': 12, 'уникален': 13, 'горы': 14, 'пробуждает': 15, 'вокруг': 16, 'природе': 17, 'весна': 18, 'запахи': 19, 'прогулка': 20, 'спокойствия': 21, 'атмосферу': 22, 'полна': 23, 'природа': 24, 'волшебную': 25, 'красоту': 26, 'найти': 27, 'каждый': 28, 'удивительных': 29, 'гармонии': 30, 'лучи': 31, 'чуде': 32, 'яркими': 33, 'озером': 34, 'приносит': 35, 'мир': 36, 'река': 37, 'радостью': 38, 'наполняют': 39, 'солнечные': 40, 'краски': 41, 'силу': 42, 'природы': 43, 'скрыта': 44, 'которая': 45, 'ощущается': 46, 'леса': 47, 'оттенками': 48, 'умиротворение': 49, 'чудес': 50, 'текет': 51, 'создавая': 52, 'сезон': 53, 'создает': 54, 'закат': 55, 'волшебный': 56, 'зелень': 57, 'повсюду': 58, 'весной': 59, 'цветы': 60, 'природу': 61, 'лесу': 62, 'своя': 63, 'которые': 64, 'окружает': 65, 'позволяет': 66, 'радуя': 67, 'распускаются': 68, 'воздух': 69, 'дарят': 70}\n",
            "Матрица TF-IDF:\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04700036292457357, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.09808292530117263, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905]\n",
            "[0.0, 0.3080654135821979, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10898102811241402, 0.0, 0.0, 0.15403270679109896, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.12602676010180824, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.04272760265870324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.12602676010180824, 0.0, 0.0, 0.0, 0.12602676010180824, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09808292530117263, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.04700036292457357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0]\n",
            "[0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10898102811241402, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.04700036292457357, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ThQxEYZpzGjr"
      }
    }
  ]
}