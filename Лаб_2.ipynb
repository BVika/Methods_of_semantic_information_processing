{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoI3Ui5QjSxZGI/0SMwG7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BVika/Methods_of_semantic_information_processing/blob/main/%D0%9B%D0%B0%D0%B1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задания"
      ],
      "metadata": {
        "id": "ZfTW7XPOKczy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Применить к текстам лемматизацию, удаление стоп слов и токенизацию по словам\n",
        "2. Реализовать Bag of Words\n",
        "3. Реализовать TF-IDF"
      ],
      "metadata": {
        "id": "r1WG8s4gKj9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка библиотек"
      ],
      "metadata": {
        "id": "vJePciSrLs0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wXDByHbLtGK",
        "outputId": "360eb54d-252e-43ca-85f2-c6058bf4e173"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE703c8vOE2q",
        "outputId": "f4f7291a-5a78-4b02-8787-8d83b2d09630"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Задание 1**\n",
        "*Применить к текстам лемматизацию, удаление стоп слов и токенизацию по словам*"
      ],
      "metadata": {
        "id": "hHZ-6jDSLA3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `на русском`\n"
      ],
      "metadata": {
        "id": "tq6y4FS3LXkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт библиотек"
      ],
      "metadata": {
        "id": "blqbYGpqOaZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymystem3 import Mystem\n",
        "from typing import List\n",
        "import pymorphy3"
      ],
      "metadata": {
        "id": "eE_27w6sOeji"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример текста\n",
        "texts = [\n",
        "    \"Я люблю программирование и изучение новых технологий.\",\n",
        "    \"Машинное обучение - это увлекательная область.\"\n",
        "]"
      ],
      "metadata": {
        "id": "0Ms1Km7wvUrj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG3fjcECKPDF",
        "outputId": "f06cd2df-a010-4c2f-de08-096432f0ba0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        }
      ],
      "source": [
        "# Загрузка необходимых ресурсов\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "m = Mystem()  # Программа MyStem производит морфологический анализ текста на русском языке\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize(text: List[str]) -> List[str]:\n",
        "    lemmas = []\n",
        "    for word in text:\n",
        "        lemmas.extend(m.lemmatize(word))  # Лемматизируем каждое слово\n",
        "        cleaned_lemmas = [lemma.strip() for lemma in lemmas if lemma.strip() and lemma not in [' ']]\n",
        "    return cleaned_lemmas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление стоп-слов и лемматизация для каждого текста\n",
        "for text in texts:\n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text)\n",
        "    # Удаление стоп-слов\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Лемматизация\n",
        "    lemmas = lemmatize(filtered_tokens)\n",
        "\n",
        "    # Результат\n",
        "    print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EVUCIq1vZNT",
        "outputId": "5e04806f-7896-4c93-bd9d-bdf2147e223e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['любить', 'программирование', 'изучение', 'новый', 'технология', '.']\n",
            "['машинный', 'обучение', '-', 'это', 'увлекательный', 'область', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `на английском`\n",
        "\n"
      ],
      "metadata": {
        "id": "O01rmpW7WCzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт библиотек"
      ],
      "metadata": {
        "id": "uQy9XYCqSvQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "X4ZNXVKtSy4p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример текста\n",
        "texts = [\n",
        "    \"I love programming and learning new technologies.\",\n",
        "    \"Machine learning is an exciting field.\"\n",
        "]"
      ],
      "metadata": {
        "id": "JCXKiDaOvRlc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка необходимых ресурсов\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Инициализация лемматизатора\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize(text: List[str]) -> List[str]:\n",
        "    lemmas = [lemmatizer.lemmatize(word) for word in text]  # Лемматизируем каждое слово\n",
        "    return lemmas\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqK9xMBaSvmR",
        "outputId": "f19998d7-793c-47dd-b99f-cd1e6d7fd1f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление стоп-слов и лемматизация для каждого текста\n",
        "for text in texts:\n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text)\n",
        "    # Удаление стоп-слов\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Лемматизация\n",
        "    lemmas = lemmatize(filtered_tokens)\n",
        "\n",
        "    # Результат\n",
        "    print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "805SqZEcvcED",
        "outputId": "c8206f67-5fb1-4225-970a-ead3a77da501"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'programming', 'learning', 'new', 'technology', '.']\n",
            "['Machine', 'learning', 'exciting', 'field', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Задание 2**\n",
        "Реализовать Bag of Words"
      ],
      "metadata": {
        "id": "nmTtkRKJWie5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "s5pcpTnPWxrY"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"Природа полна удивительных чудес, и в каждом чуде природы скрыта своя красота.\",\n",
        "    \"Леса и горы дарят нам ощущение спокойствия и гармонии, которые можно найти только в природе.\",\n",
        "    \"Птицы поют свои мелодии, и их мелодии наполняют воздух радостью и счастьем.\",\n",
        "    \"Река текет, отражая солнечные лучи и зелень вокруг, создавая волшебный мир природы.\",\n",
        "    \"Каждый сезон приносит свои краски и запахи, и каждый сезон природы уникален.\",\n",
        "    \"Цветы распускаются весной, радуя глаз яркими оттенками, и весна пробуждает природу.\",\n",
        "    \"Закат над озером создает волшебную атмосферу и умиротворение, которое ощущается в природе.\",\n",
        "    \"Прогулка по лесу позволяет ощутить силу и красоту природы, которая окружает нас повсюду.\"\n",
        "]"
      ],
      "metadata": {
        "id": "BevsUvQUvzqz"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text: List[str]) -> List[str]:\n",
        "    lemmas = []\n",
        "    for word in text:\n",
        "        lemmas.extend(m.lemmatize(word))  # Лемматизируем каждое слово\n",
        "        cleaned_lemmas = [lemma.strip() for lemma in lemmas if lemma.strip() and lemma not in [' ']]\n",
        "    return cleaned_lemmas"
      ],
      "metadata": {
        "id": "YKHmuYDoSRo9"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(texts: List[str]) -> List[List[str]]:\n",
        "\n",
        "    \"\"\"Предобработка текста, удаляющая стоп-слова и знаки препинания, возвращающая список списков токенов.\"\"\"\n",
        "    punctuation_marks = ['!', ',', '(', ')', ':', '-', '?', '.', '..', '...', '«', '»', ';', '–', '--']\n",
        "    stop_words = set(stopwords.words(\"russian\"))\n",
        "    tokenized_texts = []\n",
        "\n",
        "    for text in texts:\n",
        "\n",
        "        text = text.lower() # уриводим текст к нижнему регистру\n",
        "        # удаляем знаки препинания\n",
        "        for punctuation in punctuation_marks:\n",
        "            text = text.replace(punctuation, '')\n",
        "        tokens = text.split()# токенизируем текст\n",
        "\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words] # удаляем стоп-слова\n",
        "\n",
        "        lemmatized_tokens = lemmatize(filtered_tokens) #лемматизация\n",
        "\n",
        "        tokenized_texts.append(lemmatized_tokens)\n",
        "    return tokenized_texts\n",
        "\n",
        "\n",
        "# токинизация и лемматизация\n",
        "tokenized_texts = preprocessing(texts)\n",
        "for row in tokenized_texts:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIwfc8NhyEUr",
        "outputId": "04741b6b-e06e-449b-fac2-9aa090a49e11"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['природа', 'полный', 'удивительный', 'чудо', 'каждый', 'чудо', 'природа', 'скрытый', 'свой', 'красота']\n",
            "['лес', 'гора', 'дарить', 'мы', 'ощущение', 'спокойствие', 'гармония', 'который', 'находить', 'природа']\n",
            "['птица', 'петь', 'свой', 'мелодия', 'мелодия', 'наполнять', 'воздух', 'радость', 'счастие']\n",
            "['река', 'текет', 'отражать', 'солнечный', 'луч', 'зелень', 'вокруг', 'создавать', 'волшебный', 'мир', 'природа']\n",
            "['каждый', 'сезон', 'приносить', 'свой', 'краска', 'запах', 'каждый', 'сезон', 'природа', 'уникальный']\n",
            "['цветок', 'распускаться', 'весна', 'радовать', 'глаз', 'яркий', 'оттенок', 'весна', 'пробуждать', 'природа']\n",
            "['закат', 'озеро', 'создавать', 'волшебный', 'атмосфера', 'умиротворение', 'который', 'ощущаться', 'природа']\n",
            "['прогулка', 'лес', 'позволять', 'ощущать', 'сила', 'красота', 'природа', 'который', 'окружать', 'повсюду']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict(texts: list[list[str]]) -> dict[str, int]:\n",
        "\n",
        "    \"\"\"Создание словаря уникальных слов из текстов.\"\"\"\n",
        "    unique_words = set()\n",
        "    for text in texts:\n",
        "        unique_words.update(text)  #добавление уникальных слов в множество\n",
        "\n",
        "    return {word: index for index, word in enumerate(unique_words)}  #присвоение индексов\n",
        "\n",
        "# создание словаря уникальных слов\n",
        "word_dict = make_dict(tokenized_texts)\n",
        "print(word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8wN6bvKTYel",
        "outputId": "aefed95a-8066-4a8a-811f-c130390a93bb"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'скрытый': 0, 'распускаться': 1, 'ощущение': 2, 'создавать': 3, 'глаз': 4, 'птица': 5, 'сила': 6, 'полный': 7, 'гармония': 8, 'гора': 9, 'вокруг': 10, 'весна': 11, 'озеро': 12, 'прогулка': 13, 'ощущать': 14, 'ощущаться': 15, 'свой': 16, 'природа': 17, 'уникальный': 18, 'каждый': 19, 'чудо': 20, 'мы': 21, 'запах': 22, 'петь': 23, 'река': 24, 'мир': 25, 'оттенок': 26, 'радость': 27, 'мелодия': 28, 'лес': 29, 'приносить': 30, 'атмосфера': 31, 'краска': 32, 'дарить': 33, 'радовать': 34, 'спокойствие': 35, 'пробуждать': 36, 'умиротворение': 37, 'текет': 38, 'сезон': 39, 'удивительный': 40, 'закат': 41, 'волшебный': 42, 'яркий': 43, 'который': 44, 'зелень': 45, 'повсюду': 46, 'отражать': 47, 'наполнять': 48, 'окружать': 49, 'цветок': 50, 'луч': 51, 'находить': 52, 'позволять': 53, 'счастие': 54, 'воздух': 55, 'красота': 56, 'солнечный': 57}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_num_words(texts: list[list[str]], word_dict: dict[str, int]) -> list[list[int]]:\n",
        "\n",
        "    \"\"\"Подсчет количества вхождений слов для каждого текста.\"\"\"\n",
        "    word_counts = []\n",
        "    for text in texts:\n",
        "        count_vector = [0] * len(word_dict)  # инициализация вектора счетчиков\n",
        "        for word in text:\n",
        "            if word in word_dict:\n",
        "                count_vector[word_dict[word]] += 1\n",
        "        word_counts.append(count_vector)\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "# подсчет количества слов\n",
        "word_counts = count_num_words(tokenized_texts, word_dict)\n",
        "for row in word_counts:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCFxN3JtTh0f",
        "outputId": "36052600-b4c5-4d41-c9ee-f9fa551a1080"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Словарь уникальных слов:\", word_dict)\n",
        "print(\"Матрица Bag of Words:\")\n",
        "for row in word_counts:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AgNRk-ETjYU",
        "outputId": "a37c4282-b777-4eb7-e5c0-8fe9d7f1178c"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь уникальных слов: {'скрытый': 0, 'распускаться': 1, 'ощущение': 2, 'создавать': 3, 'глаз': 4, 'птица': 5, 'сила': 6, 'полный': 7, 'гармония': 8, 'гора': 9, 'вокруг': 10, 'весна': 11, 'озеро': 12, 'прогулка': 13, 'ощущать': 14, 'ощущаться': 15, 'свой': 16, 'природа': 17, 'уникальный': 18, 'каждый': 19, 'чудо': 20, 'мы': 21, 'запах': 22, 'петь': 23, 'река': 24, 'мир': 25, 'оттенок': 26, 'радость': 27, 'мелодия': 28, 'лес': 29, 'приносить': 30, 'атмосфера': 31, 'краска': 32, 'дарить': 33, 'радовать': 34, 'спокойствие': 35, 'пробуждать': 36, 'умиротворение': 37, 'текет': 38, 'сезон': 39, 'удивительный': 40, 'закат': 41, 'волшебный': 42, 'яркий': 43, 'который': 44, 'зелень': 45, 'повсюду': 46, 'отражать': 47, 'наполнять': 48, 'окружать': 49, 'цветок': 50, 'луч': 51, 'находить': 52, 'позволять': 53, 'счастие': 54, 'воздух': 55, 'красота': 56, 'солнечный': 57}\n",
            "Матрица Bag of Words:\n",
            "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Задание 3**\n",
        "Реализовать TF-IDF"
      ],
      "metadata": {
        "id": "cfTmQFQjv4sL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Частотность термина (TF). Измеряет, насколько часто слово встречается в документе. Рассчитывается как отношение количества вхождений слова к количеству слов в документе.\n",
        "\n",
        "Обратная частотность документа (IDF). Измеряет, насколько редким является слово в наборе документов. Рассчитывается как логарифм отношения количества документов в коллекции к количеству документов в коллекции, в которых встречается заданное слово.\n",
        "\n",
        "Общий показатель TF-IDF является произведением TF и IDF. Формула: TF-IDF = TF * IDF."
      ],
      "metadata": {
        "id": "RcaeCYS9CavF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import List, Dict\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "q6qg1OuoH8kq"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(texts: List[str]) -> List[List[str]]:\n",
        "\n",
        "    \"\"\"Предобработка текста, удаляющая стоп-слова и знаки препинания, возвращающая список списков токенов.\"\"\"\n",
        "    punctuation_marks = ['!', ',', '(', ')', ':', '-', '?', '.', '..', '...', '«', '»', ';', '–', '--']\n",
        "    stop_words = set(stopwords.words(\"russian\"))\n",
        "    tokenized_texts = []\n",
        "\n",
        "    for text in texts:\n",
        "        text = text.lower() # приводим текст к нижнему регистру\n",
        "        # удаляем знаки препинания\n",
        "        for punctuation in punctuation_marks:\n",
        "            text = text.replace(punctuation, '')\n",
        "\n",
        "        tokens = text.split()# токенизируем текст\n",
        "\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words] # удаляем стоп-слова\n",
        "\n",
        "        lemmatized_tokens = lemmatize(filtered_tokens) #лемматизация\n",
        "\n",
        "        tokenized_texts.append(lemmatized_tokens)\n",
        "    return tokenized_texts\n",
        "\n",
        "\n",
        "tokenized_texts = preprocessing(texts)\n",
        "for row in tokenized_texts:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kjUGUDhzs2j",
        "outputId": "f9731323-253e-4a51-b460-a2a9a033d3ef"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['природа', 'полный', 'удивительный', 'чудо', 'каждый', 'чудо', 'природа', 'скрытый', 'свой', 'красота']\n",
            "['лес', 'гора', 'дарить', 'мы', 'ощущение', 'спокойствие', 'гармония', 'который', 'находить', 'природа']\n",
            "['птица', 'петь', 'свой', 'мелодия', 'мелодия', 'наполнять', 'воздух', 'радость', 'счастие']\n",
            "['река', 'текет', 'отражать', 'солнечный', 'луч', 'зелень', 'вокруг', 'создавать', 'волшебный', 'мир', 'природа']\n",
            "['каждый', 'сезон', 'приносить', 'свой', 'краска', 'запах', 'каждый', 'сезон', 'природа', 'уникальный']\n",
            "['цветок', 'распускаться', 'весна', 'радовать', 'глаз', 'яркий', 'оттенок', 'весна', 'пробуждать', 'природа']\n",
            "['закат', 'озеро', 'создавать', 'волшебный', 'атмосфера', 'умиротворение', 'который', 'ощущаться', 'природа']\n",
            "['прогулка', 'лес', 'позволять', 'ощущать', 'сила', 'красота', 'природа', 'который', 'окружать', 'повсюду']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict(texts: List[List[str]]) -> Dict[str, int]:\n",
        "\n",
        "    \"\"\"Создание словаря уникальных слов из текстов.\"\"\"\n",
        "    unique_words = set()\n",
        "    for text in texts:\n",
        "        unique_words.update(text)\n",
        "\n",
        "    return {word: index for index, word in enumerate(unique_words)}\n",
        "\n",
        "# создание словаря уникальных слов\n",
        "word_dict = make_dict(tokenized_texts)\n",
        "print(word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDe285MzGHgm",
        "outputId": "5fd40799-47b5-4fc8-dfc7-b25942e029d6"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'скрытый': 0, 'распускаться': 1, 'ощущение': 2, 'создавать': 3, 'глаз': 4, 'птица': 5, 'сила': 6, 'полный': 7, 'гармония': 8, 'гора': 9, 'вокруг': 10, 'весна': 11, 'озеро': 12, 'прогулка': 13, 'ощущать': 14, 'ощущаться': 15, 'свой': 16, 'природа': 17, 'уникальный': 18, 'каждый': 19, 'чудо': 20, 'мы': 21, 'запах': 22, 'петь': 23, 'река': 24, 'мир': 25, 'оттенок': 26, 'радость': 27, 'мелодия': 28, 'лес': 29, 'приносить': 30, 'атмосфера': 31, 'краска': 32, 'дарить': 33, 'радовать': 34, 'спокойствие': 35, 'пробуждать': 36, 'умиротворение': 37, 'текет': 38, 'сезон': 39, 'удивительный': 40, 'закат': 41, 'волшебный': 42, 'яркий': 43, 'который': 44, 'зелень': 45, 'повсюду': 46, 'отражать': 47, 'наполнять': 48, 'окружать': 49, 'цветок': 50, 'луч': 51, 'находить': 52, 'позволять': 53, 'счастие': 54, 'воздух': 55, 'красота': 56, 'солнечный': 57}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tf(texts: List[List[str]], word_dict: Dict[str, int]) -> List[List[float]]:\n",
        "\n",
        "    \"\"\"Подсчет TF для каждого текста.\"\"\"\n",
        "    tf_matrix = []\n",
        "    for text in texts:\n",
        "        count_vector = [0] * len(word_dict)\n",
        "        for word in text:\n",
        "            if word in word_dict:\n",
        "                count_vector[word_dict[word]] += 1\n",
        "\n",
        "\n",
        "        total_words = len(text)\n",
        "        tf_vector = [count / total_words for count in count_vector]\n",
        "        tf_matrix.append(tf_vector)\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "\n",
        "tf_matrix = compute_tf(tokenized_texts, word_dict)\n",
        "for row in tf_matrix:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5ceJ-TpFoEs",
        "outputId": "c500fc0a-ec40-44c1-8307-934cdbd84291"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.2, 0.0, 0.1, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0]\n",
            "[0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.1111111111111111, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.1111111111111111, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.09090909090909091, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.2, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.1111111111111111, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.1111111111111111, 0.1111111111111111, 0.0, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_idf(texts: List[List[str]], word_dict: Dict[str, int]) -> List[float]:\n",
        "\n",
        "    \"\"\"Подсчет IDF для слов.\"\"\"\n",
        "    idf_vector = []\n",
        "    num_docs = len(texts)\n",
        "\n",
        "    for word in word_dict.keys():\n",
        "        count = sum(1 for text in texts if word in text) #возвращает количество документов, в которых присутствует данное слово. Если слово не встречается ни в одном документе, count будет равно 0.\n",
        "        idf = math.log(num_docs / (count + 1))  # добавляем 1 для избежания деления на ноль\n",
        "        idf_vector.append(idf)\n",
        "\n",
        "    return idf_vector\n",
        "\n",
        "\n",
        "\n",
        "idf_vector = compute_idf(tokenized_texts, word_dict)\n",
        "print(idf_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4L9F1SGFtOm",
        "outputId": "1adf69d1-83bd-493b-c821-9030fc0828b0"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 0.9808292530117262, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 0.6931471805599453, 0.0, 1.3862943611198906, 0.9808292530117262, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 0.9808292530117262, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 0.9808292530117262, 1.3862943611198906, 0.6931471805599453, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 1.3862943611198906, 0.9808292530117262, 1.3862943611198906]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(tf_matrix: List[List[float]], idf_vector: List[float]) -> List[List[float]]:\n",
        "\n",
        "    \"\"\"Подсчет TF-IDF на основе TF и IDF.\"\"\"\n",
        "    tfidf_matrix = []\n",
        "\n",
        "    for tf_vector in tf_matrix:\n",
        "        tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n",
        "        tfidf_matrix.append(tfidf_vector)\n",
        "    return tfidf_matrix\n",
        "\n",
        "\n",
        "tfidf_matrix = compute_tfidf(tf_matrix, idf_vector)\n",
        "for row in tfidf_matrix:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZMuJoMcFvsG",
        "outputId": "4afa50ac-6b5c-41a2-c335-414f77c88b88"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06931471805599453, 0.0, 0.0, 0.09808292530117263, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09808292530117263, 0.0]\n",
            "[0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09808292530117263, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06931471805599453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07701635339554948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.15403270679109896, 0.3080654135821979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.15403270679109896, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.08916629572833874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.08916629572833874, 0.0, 0.0, 0.12602676010180824, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06931471805599453, 0.0, 0.13862943611198905, 0.19616585060234526, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.10898102811241402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.15403270679109896, 0.10898102811241402, 0.0, 0.07701635339554948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09808292530117263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06931471805599453, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.09808292530117263, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Словарь уникальных слов:\", word_dict)\n",
        "print(\"Матрица TF-IDF:\")\n",
        "for row in tfidf_matrix:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NFI4JJBH2Ma",
        "outputId": "fcb9b32b-85f5-4644-836c-847d3fafbb7d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь уникальных слов: {'скрытый': 0, 'распускаться': 1, 'ощущение': 2, 'создавать': 3, 'глаз': 4, 'птица': 5, 'сила': 6, 'полный': 7, 'гармония': 8, 'гора': 9, 'вокруг': 10, 'весна': 11, 'озеро': 12, 'прогулка': 13, 'ощущать': 14, 'ощущаться': 15, 'свой': 16, 'природа': 17, 'уникальный': 18, 'каждый': 19, 'чудо': 20, 'мы': 21, 'запах': 22, 'петь': 23, 'река': 24, 'мир': 25, 'оттенок': 26, 'радость': 27, 'мелодия': 28, 'лес': 29, 'приносить': 30, 'атмосфера': 31, 'краска': 32, 'дарить': 33, 'радовать': 34, 'спокойствие': 35, 'пробуждать': 36, 'умиротворение': 37, 'текет': 38, 'сезон': 39, 'удивительный': 40, 'закат': 41, 'волшебный': 42, 'яркий': 43, 'который': 44, 'зелень': 45, 'повсюду': 46, 'отражать': 47, 'наполнять': 48, 'окружать': 49, 'цветок': 50, 'луч': 51, 'находить': 52, 'позволять': 53, 'счастие': 54, 'воздух': 55, 'красота': 56, 'солнечный': 57}\n",
            "Матрица TF-IDF:\n",
            "[0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06931471805599453, 0.0, 0.0, 0.09808292530117263, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09808292530117263, 0.0]\n",
            "[0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09808292530117263, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06931471805599453, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07701635339554948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.15403270679109896, 0.3080654135821979, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.15403270679109896, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.08916629572833874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.08916629572833874, 0.0, 0.0, 0.12602676010180824, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.12602676010180824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12602676010180824]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06931471805599453, 0.0, 0.13862943611198905, 0.19616585060234526, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2772588722239781, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.10898102811241402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15403270679109896, 0.0, 0.0, 0.0, 0.15403270679109896, 0.10898102811241402, 0.0, 0.07701635339554948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13862943611198905, 0.13862943611198905, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09808292530117263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06931471805599453, 0.0, 0.13862943611198905, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.0, 0.13862943611198905, 0.0, 0.0, 0.09808292530117263, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ThQxEYZpzGjr"
      }
    }
  ]
}